{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c7cce84",
   "metadata": {},
   "source": [
    "### Preliminaries\n",
    "In this section, we import the required libraries and initialize standard transformations necessary for loading datasets. It is worth mentioning that certain models require input normalization, while others do not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be88b7bf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4698212f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f177ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from TextToConcept import TextToConcept\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab43a456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Number of GPUs available:  1\n",
      "GPU Name: NVIDIA GeForce RTX 4090\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1'  # GPU I\n",
    "\n",
    "# Check if CUDA is available and list available CUDA devices\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Number of GPUs available: \", torch.cuda.device_count())\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d06c9913",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dc3451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_transform_without_normalization = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(224),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor()])\n",
    "\n",
    "\n",
    "std_transform_with_normalization = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(224),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(), \n",
    "    torchvision.transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa435299",
   "metadata": {},
   "source": [
    "### Resnet50\n",
    "In this part, we load Resnet50 model.\n",
    "In order to use ``TextToConcept`` framework, model should implement these functions/attributes:\n",
    "+ ``forward_features(x)`` that takes a tensor as the input and outputs the representation (features) of input $x$ when it is passed through the model.\n",
    "+ ``get_normalizer`` should be the normalizer that the models uses to preprocess the input. e.g., Resnet18, uses standard ImageNet normalizer.\n",
    "+ Attribute ``has_normalizer`` should be `True` when normalizer is need for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28220504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/umair.nawaz/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('facebookresearch/dino:main', 'dino_vitb16')\n",
    "\n",
    "encoder = torch.nn.Sequential(*list(model.children())[:-2])\n",
    "model.forward_features = lambda x : encoder(x)\n",
    "model.get_normalizer = torchvision.transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "model.has_normalizer = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d71d09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/umair.nawaz/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Local path to the cloned dino repository\n",
    "local_repo_path = '/home/umair.nawaz/Research_Work/Submission/AgriClip/Weights/dino_pretrain.pth'  # Replace with your local path\n",
    "\n",
    "model = torch.hub.load('facebookresearch/dino:main', 'dino_resnet50', pretrained=True)\n",
    "model.load_state_dict(torch.load(local_repo_path , weights_only=True))\n",
    "\n",
    "# Load the dino_resnet50 model from the local repo\n",
    "# model = torch.hub.load(local_repo_path, 'dino_resnet50', pretrained=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d57c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "615d4dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load DINO Vision Transformer model\n",
    "# model = torch.hub.load('facebookresearch/dino:main', 'dino_vitb16', pretrained=True)\n",
    "# model = torch.hub.load('facebookresearch/dino:main', 'dino_resnet50', pretrained=True)\n",
    "\n",
    "def forward_features(x):\n",
    "    # Assuming the model returns the last layer's output which might include a classification token or similar\n",
    "    features = model(x)  # This call should be adjusted based on the actual output format\n",
    "    # You might need to process the features here, e.g., selecting the right tensor or applying global pooling\n",
    "    return features.squeeze()  # Adjust this based on the actual structure of your features\n",
    "\n",
    "\n",
    "# Attach the custom forward method to the model\n",
    "model.forward_features = forward_features\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "model.get_normalizer = torchvision.transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "model.has_normalizer = True\n",
    "\n",
    "# Create a dummy input tensor\n",
    "input_tensor = torch.randn(1, 3, 224, 224)  # Adjust size if needed\n",
    "\n",
    "# Normalize the input\n",
    "normalized_input = model.get_normalizer(input_tensor)\n",
    "\n",
    "# Get features\n",
    "features = model.forward_features(normalized_input)\n",
    "print(features.shape)  # Check the shape of the output features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f781f703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fe6c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1da8df15",
   "metadata": {},
   "source": [
    "### Linear Aligner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4b4fe7",
   "metadata": {},
   "source": [
    "<b>Initiating Text-To-Concept Object</b><br>\n",
    "In this section, we initiate ``TextToConcept`` object which turns the vision encoder (e.g., Resnet50) into a model capable of integrating language and vision. By doing so, we enable the utilization of certain abilities present in vision-language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "527a1b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_concept = TextToConcept(model, 'Dino')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01c48fb",
   "metadata": {},
   "source": [
    "We can either train the aligner or load an existing one.\n",
    "\n",
    "#### Training Linear Aligner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce244e3",
   "metadata": {},
   "source": [
    "<b>Loading ImageNet Dataset to Train the Aligner</b><br>\n",
    "We note that even $20\\%$ of ImageNet training samples suffices for training an effective linear aligner. \n",
    "We refer to Appendix A of our paper for more details on sample efficiency of linear alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1e82a18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading imagenet dataset to train aligner.\n",
    "dset = torchvision.datasets.ImageFolder(root='/share/sdb/umairnawaz/My_Surgical/down-data/',\n",
    "                                    #  split='train',\n",
    "                                     transform=std_transform_with_normalization)\n",
    "\n",
    "# 20% of images are fairly enough.\n",
    "num_of_samples = int(0.99 * len(dset))\n",
    "dset = torch.utils.data.Subset(dset, np.random.choice(np.arange(len(dset)), num_of_samples, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dbff7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248638"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65e6389",
   "metadata": {},
   "source": [
    "<b>Training the Linear Aligner</b><br>\n",
    "After loading the object, we need to train the aligner.\n",
    "+ In order to train the aligner, ``train_linear_aligner`` should be called which obtains representations of the given model (e.g., Resnet50) on ``dset`` as well that of a vision-language model such as CLIP. These representations can also be loaded. Next, this function solves the linear transformation and obtain optimal alignment from model's space to vision-language space.\n",
    "+ By calling the function ``save_linear_aligner``, linear aliger will be stored which can be utilized later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e492de01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading representations ...\n",
      "Training linear aligner ...\n",
      "Linear alignment: ((248638, 2048)) --> ((248638, 512)).\n",
      "Initial MSE, R^2: 5.796, -0.289\n",
      "Epoch number, loss: 0, 1.487\n",
      "Epoch number, loss: 1, 1.223\n",
      "Epoch number, loss: 2, 1.153\n",
      "Epoch number, loss: 3, 1.113\n",
      "Epoch number, loss: 4, 1.085\n",
      "Epoch number, loss: 5, 1.064\n",
      "Epoch number, loss: 6, 1.047\n",
      "Epoch number, loss: 7, 1.033\n",
      "Epoch number, loss: 8, 1.021\n",
      "Epoch number, loss: 9, 1.011\n",
      "Epoch number, loss: 10, 1.001\n",
      "Epoch number, loss: 11, 0.994\n",
      "Epoch number, loss: 12, 0.986\n",
      "Epoch number, loss: 13, 0.980\n",
      "Epoch number, loss: 14, 0.974\n",
      "Epoch number, loss: 15, 0.969\n",
      "Epoch number, loss: 16, 0.964\n",
      "Epoch number, loss: 17, 0.959\n",
      "Epoch number, loss: 18, 0.955\n",
      "Epoch number, loss: 19, 0.951\n",
      "Epoch number, loss: 20, 0.948\n",
      "Epoch number, loss: 21, 0.944\n",
      "Epoch number, loss: 22, 0.941\n",
      "Epoch number, loss: 23, 0.938\n",
      "Epoch number, loss: 24, 0.935\n",
      "Epoch number, loss: 25, 0.933\n",
      "Epoch number, loss: 26, 0.930\n",
      "Epoch number, loss: 27, 0.928\n",
      "Epoch number, loss: 28, 0.925\n",
      "Epoch number, loss: 29, 0.923\n",
      "Epoch number, loss: 30, 0.921\n",
      "Epoch number, loss: 31, 0.919\n",
      "Epoch number, loss: 32, 0.917\n",
      "Epoch number, loss: 33, 0.915\n",
      "Epoch number, loss: 34, 0.913\n",
      "Epoch number, loss: 35, 0.912\n",
      "Epoch number, loss: 36, 0.910\n",
      "Epoch number, loss: 37, 0.909\n",
      "Epoch number, loss: 38, 0.907\n",
      "Epoch number, loss: 39, 0.906\n",
      "Epoch number, loss: 40, 0.904\n",
      "Epoch number, loss: 41, 0.903\n",
      "Epoch number, loss: 42, 0.901\n",
      "Epoch number, loss: 43, 0.900\n",
      "Epoch number, loss: 44, 0.899\n",
      "Epoch number, loss: 45, 0.898\n",
      "Epoch number, loss: 46, 0.897\n",
      "Epoch number, loss: 47, 0.895\n",
      "Epoch number, loss: 48, 0.894\n",
      "Epoch number, loss: 49, 0.893\n",
      "Epoch number, loss: 50, 0.892\n",
      "Epoch number, loss: 51, 0.891\n",
      "Epoch number, loss: 52, 0.890\n",
      "Epoch number, loss: 53, 0.889\n",
      "Epoch number, loss: 54, 0.888\n",
      "Epoch number, loss: 55, 0.887\n",
      "Epoch number, loss: 56, 0.887\n",
      "Epoch number, loss: 57, 0.886\n",
      "Epoch number, loss: 58, 0.885\n",
      "Epoch number, loss: 59, 0.884\n",
      "Epoch number, loss: 60, 0.883\n",
      "Epoch number, loss: 61, 0.882\n",
      "Epoch number, loss: 62, 0.882\n",
      "Epoch number, loss: 63, 0.881\n",
      "Epoch number, loss: 64, 0.880\n",
      "Epoch number, loss: 65, 0.880\n",
      "Epoch number, loss: 66, 0.879\n",
      "Epoch number, loss: 67, 0.878\n",
      "Epoch number, loss: 68, 0.877\n",
      "Epoch number, loss: 69, 0.877\n",
      "Final MSE, R^2 = 0.877, 0.805\n"
     ]
    }
   ],
   "source": [
    "path1 = '/share/sdb/umairnawaz/T2C/Text-to-concept/Aligned_Models/DINO/1/DINO.npy'\n",
    "path2 = '/share/sdb/umairnawaz/T2C/Text-to-concept/Aligned_Models/CLIP/1/CLIP.npy'\n",
    "\n",
    "text_to_concept.train_linear_aligner(dset,\n",
    "                                     save_reps = False ,load_reps=True,path_to_model=path1, path_to_clip_model=path2, epochs = 70)\n",
    "\n",
    "\n",
    "text_to_concept.save_linear_aligner('./Aligned_Models/Agri_Dino_aligner_V7_3_vitb16.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02221daa",
   "metadata": {},
   "source": [
    "<b>Loading the Linear Aligner</b><br>\n",
    "We can also use an already existing linear aligner, to do so, we use the function ``load_linear_aligner``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e3b38e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_concept.load_linear_aligner('/home/umair.nawaz/Research_Work/Submission/AgriClip/Weights/Aligned_Models/Agri_Dino_aligner_V6.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aae71d",
   "metadata": {},
   "source": [
    "### Zero-shot Classifier\n",
    "We note that CIFAR-10 is a <i>$10$-way</i> classification problem. \n",
    "We use prompts of the form `a pixelated of {c}` to get appropriate concepts in vision-language space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3acb2674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cifar_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46e11502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['boron', 'calcium', 'healthy', 'iron', 'magnesium', 'manganese', 'potassium', 'sulphur', 'zinc']\n"
     ]
    }
   ],
   "source": [
    "# cifar = torchvision.datasets.CIFAR10(root='/home/umair.nawaz/Research_Work/Main-DATA/My_Surgical/downstream/crops/dataset_22/',\n",
    "#                                      download=False,\n",
    "#                                      train=False,\n",
    "#                                      transform=None)\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "directory_path = '/home/umair.nawaz/Research_Work/Main-DATA/My_Surgical/downstream/crops/dataset_22'\n",
    "\n",
    "# Specify the directory where CIFAR-10 images are stored\n",
    "# datapath = '/home/umair.nawaz/Research_Work/Main-DATA/My_Surgical/downstream/crops/dataset_22'\n",
    "\n",
    "# Create the dataset using the ImageFolder class\n",
    "cifar_dataset = ImageFolder(root=directory_path,\n",
    "                            transform=std_transform_with_normalization)\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "# 20% of images are fairly enough.\n",
    "# num_of_samples = int(0.5 * len(cifar_dataset))\n",
    "# cifar_dset = torch.utils.data.Subset(cifar_dataset, np.random.choice(np.arange(len(cifar_dataset)), num_of_samples, replace=False))\n",
    "\n",
    "####\n",
    "\n",
    "\n",
    "loader = torch.utils.data.DataLoader(cifar_dataset, batch_size=32, shuffle=True, num_workers=8)\n",
    "\n",
    "# Get class name mappings\n",
    "idx_to_class = {v: k for k, v in cifar_dataset.class_to_idx.items()}\n",
    "# Convert dictionary values to a list, sorted by key\n",
    "class_names_list = [idx_to_class[i] for i in sorted(idx_to_class.keys())]\n",
    "print(class_names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20efdd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_zeroshot_classifier = text_to_concept.get_zero_shot_classifier(class_names_list,\n",
    "                                                                     prompts=['a photo contain {} deficiency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c01e7538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cifar_zeroshot_classifier = text_to_concept.get_zero_shot_classifier(cifar_classes,\n",
    "#                                                                      prompts=['a pixelated photo of a {}'])a type of fish named {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b02859f",
   "metadata": {},
   "source": [
    "### Zero-shot performance on CIFAR-10\n",
    "After loading CIFAR-10, we use `cifar_zeroshot_classifier(x)` to get logits of the classification problem when input $x$ is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6492d680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cifar = torchvision.datasets.CIFAR10(root='data/',\n",
    "#                                      download=True,\n",
    "#                                      train=False,\n",
    "#                                      transform=std_transform_with_normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "571e6044",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/97 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [00:06<00:00, 14.55it/s]\n"
     ]
    }
   ],
   "source": [
    "# loader = torch.utils.data.DataLoader(cifar, batch_size=16, shuffle=True, num_workers=8)\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(loader):\n",
    "        x, y = data[:2]\n",
    "        x = x.to(device)\n",
    "\n",
    "        try:\n",
    "            # Attempt to compute the outputs\n",
    "            outputs = cifar_zeroshot_classifier(x).detach().cpu()\n",
    "            _, predicted = outputs.max(1)  # Find the index of the max log-probability\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y).sum().item()\n",
    "        except IndexError as e:\n",
    "            print(f\"Error processing batch: {e}\")\n",
    "            print(f\"x.shape: {x.shape}, y: {y}\")\n",
    "            continue  # Skip this batch and continue with the next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c086a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Zeroshot Accuracy: 14.19'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'Zeroshot Accuracy: {100.*correct/total:.2f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b7222b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148cfa73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6399b408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b1428f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
