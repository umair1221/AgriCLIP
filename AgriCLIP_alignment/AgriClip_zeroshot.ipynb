{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4698212f",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f177ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from TextToConcept import TextToConcept\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab43a456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Number of GPUs available:  1\n",
      "GPU Name: Quadro RTX 6000\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# In case if multiple GPUs\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1'  # GPU I\n",
    "\n",
    "# Check if CUDA is available and list available CUDA devices\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Number of GPUs available: \", torch.cuda.device_count())\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d06c9913",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dc3451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_transform_without_normalization = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(224),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor()])\n",
    "\n",
    "\n",
    "std_transform_with_normalization = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(224),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(), \n",
    "    torchvision.transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa435299",
   "metadata": {},
   "source": [
    "#\n",
    "In this part, we load the DINO model. We also use ``forward_features(x)`` that takes a tensor as the input and outputs the representation (features) of input $x$ when it is passed through the model and ``get_normalizer``, which is the normalizer that the models uses to preprocess the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d71d09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/umair.nawaz/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    }
   ],
   "source": [
    "# Local path to the cloned dino repository\n",
    "local_repo_path = '../Weights/dino_pretrain.pth'  # Replace with your local path\n",
    "\n",
    "model = torch.hub.load('facebookresearch/dino:main', 'dino_resnet50', pretrained=True)\n",
    "model.load_state_dict(torch.load(local_repo_path , weights_only=True))\n",
    "\n",
    "def forward_features(x):\n",
    "    # Assuming the model returns the last layer's output which might include a classification token or similar\n",
    "    features = model(x)  # This call should be adjusted based on the actual output format\n",
    "    # You might need to process the features here, e.g., selecting the right tensor or applying global pooling\n",
    "    return features.squeeze()  # Adjust this based on the actual structure of your features\n",
    "\n",
    "\n",
    "# Attach the custom forward method to the model\n",
    "model.forward_features = forward_features\n",
    "\n",
    "model.get_normalizer = torchvision.transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "model.has_normalizer = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4b4fe7",
   "metadata": {},
   "source": [
    "<b>Initiating Text-To-Concept Object</b><br>\n",
    "In this section, we initiate ``TextToConcept`` object which turns the vision encoder into a model capable of integrating language and vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "527a1b91",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m text_to_concept \u001b[39m=\u001b[39m TextToConcept(model, \u001b[39m'\u001b[39;49m\u001b[39mDino\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/Research_Work/Submission/AgriCLIP/AgriCLIP alignment/TextToConcept.py:63\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, model, model_name)\u001b[0m\n",
      "File \u001b[0;32m~/Research_Work/Submission/AgriCLIP/AgriCLIP alignment/TextToConcept.py:28\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, mtype)\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/t2c/lib/python3.10/site-packages/clip/clip.py:139\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, device, jit, download_root)\u001b[0m\n\u001b[1;32m    136\u001b[0m         state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(opened_file, map_location\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m jit:\n\u001b[0;32m--> 139\u001b[0m     model \u001b[39m=\u001b[39m build_model(state_dict \u001b[39mor\u001b[39;49;00m model\u001b[39m.\u001b[39;49mstate_dict())\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m    140\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mstr\u001b[39m(device) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    141\u001b[0m         model\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/.conda/envs/t2c/lib/python3.10/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/.conda/envs/t2c/lib/python3.10/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    782\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/t2c/lib/python3.10/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    782\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/t2c/lib/python3.10/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[39m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/t2c/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[39m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(\n\u001b[1;32m   1161\u001b[0m         device,\n\u001b[1;32m   1162\u001b[0m         dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1163\u001b[0m         non_blocking,\n\u001b[1;32m   1164\u001b[0m     )\n\u001b[1;32m   1165\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mstr\u001b[39m(e) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCannot copy out of meta tensor; no data!\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "text_to_concept = TextToConcept(model, 'Dino')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02221daa",
   "metadata": {},
   "source": [
    "<b>Loading the Linear Aligner</b><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e3b38e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_concept.load_linear_aligner('/home/umair.nawaz/Research_Work/Submission/AgriClip/Weights/Aligned_Models/Agri_Dino_aligner_V6.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aae71d",
   "metadata": {},
   "source": [
    "### Zero-shot Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46e11502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['boron', 'calcium', 'healthy', 'iron', 'magnesium', 'manganese', 'potassium', 'sulphur', 'zinc']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# Specify the directory where images are stored\n",
    "directory_path = '/home/umair.nawaz/Research_Work/Main-DATA/My_Surgical/downstream-final/Banana Deficiency'\n",
    "\n",
    "# Create the dataset using the ImageFolder class\n",
    "dataset = ImageFolder(root=directory_path,\n",
    "                            transform=std_transform_with_normalization)\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "# 20% of images are fairly enough.\n",
    "# num_of_samples = int(0.5 * len(cifar_dataset))\n",
    "# cifar_dset = torch.utils.data.Subset(cifar_dataset, np.random.choice(np.arange(len(cifar_dataset)), num_of_samples, replace=False))\n",
    "\n",
    "####\n",
    "\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True, num_workers=8)\n",
    "\n",
    "# Get class name mappings\n",
    "idx_to_class = {v: k for k, v in dataset.class_to_idx.items()}\n",
    "# Convert dictionary values to a list, sorted by key\n",
    "class_names_list = [idx_to_class[i] for i in sorted(idx_to_class.keys())]\n",
    "print(class_names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20efdd02",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_to_concept' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cifar_zeroshot_classifier \u001b[39m=\u001b[39m text_to_concept\u001b[39m.\u001b[39mget_zero_shot_classifier(class_names_list,\n\u001b[1;32m      2\u001b[0m                                                                      prompts\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39ma photo contain \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m deficiency\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_to_concept' is not defined"
     ]
    }
   ],
   "source": [
    "cifar_zeroshot_classifier = text_to_concept.get_zero_shot_classifier(class_names_list,\n",
    "                                                                     prompts=['a photo contain {} deficiency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c01e7538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cifar_zeroshot_classifier = text_to_concept.get_zero_shot_classifier(cifar_classes,\n",
    "#                                                                      prompts=['a pixelated photo of a {}'])a type of fish named {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b02859f",
   "metadata": {},
   "source": [
    "### Zero-shot performance on CIFAR-10\n",
    "After loading CIFAR-10, we use `cifar_zeroshot_classifier(x)` to get logits of the classification problem when input $x$ is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6492d680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cifar = torchvision.datasets.CIFAR10(root='data/',\n",
    "#                                      download=True,\n",
    "#                                      train=False,\n",
    "#                                      transform=std_transform_with_normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "571e6044",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/97 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [00:06<00:00, 14.55it/s]\n"
     ]
    }
   ],
   "source": [
    "# loader = torch.utils.data.DataLoader(cifar, batch_size=16, shuffle=True, num_workers=8)\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(loader):\n",
    "        x, y = data[:2]\n",
    "        x = x.to(device)\n",
    "\n",
    "        try:\n",
    "            # Attempt to compute the outputs\n",
    "            outputs = cifar_zeroshot_classifier(x).detach().cpu()\n",
    "            _, predicted = outputs.max(1)  # Find the index of the max log-probability\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y).sum().item()\n",
    "        except IndexError as e:\n",
    "            print(f\"Error processing batch: {e}\")\n",
    "            print(f\"x.shape: {x.shape}, y: {y}\")\n",
    "            continue  # Skip this batch and continue with the next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c086a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Zeroshot Accuracy: 14.19'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'Zeroshot Accuracy: {100.*correct/total:.2f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b7222b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148cfa73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6399b408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b1428f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
