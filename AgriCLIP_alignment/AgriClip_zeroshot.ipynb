{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4698212f",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f177ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/umair.nawaz/.conda/envs/t2c/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from TextToConcept import TextToConcept\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab43a456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Number of GPUs available:  1\n",
      "GPU Name: NVIDIA GeForce RTX 4090\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# In case if multiple GPUs\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1'  # GPU I\n",
    "\n",
    "# Check if CUDA is available and list available CUDA devices\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Number of GPUs available: \", torch.cuda.device_count())\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d06c9913",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dc3451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_transform_without_normalization = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(224),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor()])\n",
    "\n",
    "\n",
    "std_transform_with_normalization = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(224),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(), \n",
    "    torchvision.transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa435299",
   "metadata": {},
   "source": [
    "#\n",
    "In this part, we load the DINO model. We also use ``forward_features(x)`` that takes a tensor as the input and outputs the representation (features) of input $x$ when it is passed through the model and ``get_normalizer``, which is the normalizer that the models uses to preprocess the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d71d09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/umair.nawaz/.cache/torch/hub/facebookresearch_dino_main\n",
      "/home/umair.nawaz/.conda/envs/t2c/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/umair.nawaz/.conda/envs/t2c/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Local path to the cloned dino repository\n",
    "local_repo_path = '../Weights/dino_pretrain.pth'  # Replace with your local path\n",
    "\n",
    "model = torch.hub.load('facebookresearch/dino:main', 'dino_resnet50', pretrained=True)\n",
    "model.load_state_dict(torch.load(local_repo_path , weights_only=True))\n",
    "\n",
    "def forward_features(x):\n",
    "    # Assuming the model returns the last layer's output which might include a classification token or similar\n",
    "    features = model(x)  # This call should be adjusted based on the actual output format\n",
    "    # You might need to process the features here, e.g., selecting the right tensor or applying global pooling\n",
    "    return features.squeeze()  # Adjust this based on the actual structure of your features\n",
    "\n",
    "\n",
    "# Attach the custom forward method to the model\n",
    "model.forward_features = forward_features\n",
    "\n",
    "model.get_normalizer = torchvision.transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "model.has_normalizer = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4b4fe7",
   "metadata": {},
   "source": [
    "<b>Initiating Text-To-Concept Object</b><br>\n",
    "In this section, we initiate ``TextToConcept`` object which turns the vision encoder into a model capable of integrating language and vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "527a1b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_concept = TextToConcept(model, 'Dino')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02221daa",
   "metadata": {},
   "source": [
    "<b>Loading the Linear Aligner</b><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e3b38e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/umair.nawaz/Research_Work/Submission/AgriCLIP/AgriCLIP_alignment/LinearAligner.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  aligner_dict = torch.load(path_to_load)\n"
     ]
    }
   ],
   "source": [
    "text_to_concept.load_linear_aligner('/home/umair.nawaz/Research_Work/Submission/AgriCLIP/Weights/Aligned_Models/Agri_Dino_aligner_DPT_CPT.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aae71d",
   "metadata": {},
   "source": [
    "### Zero-shot Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46e11502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['boron', 'calcium', 'healthy', 'iron', 'magnesium', 'manganese', 'potassium', 'sulphur', 'zinc']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# Specify the directory where images are stored\n",
    "directory_path = '/home/umair.nawaz/Research_Work/Main-DATA/My_Surgical/downstream-final/Banana Deficiency'\n",
    "\n",
    "# Create the dataset using the ImageFolder class\n",
    "dataset = ImageFolder(root=directory_path,\n",
    "                            transform=std_transform_with_normalization)\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "# 50% of images are fairly enough.\n",
    "# num_of_samples = int(0.5 * len(cifar_dataset))\n",
    "# cifar_dset = torch.utils.data.Subset(cifar_dataset, np.random.choice(np.arange(len(cifar_dataset)), num_of_samples, replace=False))\n",
    "\n",
    "####\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True, num_workers=8)\n",
    "\n",
    "# Get class name mappings\n",
    "idx_to_class = {v: k for k, v in dataset.class_to_idx.items()}\n",
    "# Convert dictionary values to a list, sorted by key\n",
    "class_names_list = [idx_to_class[i] for i in sorted(idx_to_class.keys())]\n",
    "print(class_names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20efdd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_zeroshot_classifier = text_to_concept.get_zero_shot_classifier(class_names_list,\n",
    "                                                                     prompts=['a photo contain {} deficiency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "571e6044",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/97 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [00:03<00:00, 30.33it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(loader):\n",
    "        x, y = data[:2]\n",
    "        x = x.to(device)\n",
    "\n",
    "        try:\n",
    "            # Attempt to compute the outputs\n",
    "            outputs = cifar_zeroshot_classifier(x).detach().cpu()\n",
    "            _, predicted = outputs.max(1)  # Find the index of the max log-probability\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y).sum().item()\n",
    "        except IndexError as e:\n",
    "            print(f\"Error processing batch: {e}\")\n",
    "            print(f\"x.shape: {x.shape}, y: {y}\")\n",
    "            continue  # Skip this batch and continue with the next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c086a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Zeroshot Accuracy: 23.55'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'Zeroshot Accuracy: {100.*correct/total:.2f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b1428f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
